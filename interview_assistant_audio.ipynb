{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunafita/Interview-Assistant---Audio-Interview/blob/main/interview_assistant_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vssaTwDlHZ2B"
      },
      "source": [
        "# **üß† AI-Powered Interview Assistant with Audio (English Practice)**\n",
        "\n",
        "Welcome! This notebook helps you simulate job interviews in English using your own resume and a job description. It uses GPT-4 to generate interview questions, Text-to-Speech (TTS) to speak the questions, and Whisper (Speech-to-Text) to transcribe your audio responses. The assistant then evaluates your answers and gives feedback or follow-up questions ‚Äî just like a real interview.\n",
        "\n",
        "# üéØ Objectives\n",
        "\n",
        "* Practice speaking English in a job interview context\n",
        "* Receive realistic interview questions based on your resume and a job description\n",
        "* Respond with your voice, not just typing\n",
        "* Get feedback on your answers from AI\n",
        "* Prepare more confidently for remote or live interviews\n",
        "\n",
        "# üîß Tools Used\n",
        "\n",
        "* OpenAI GPT-4 ‚Äì for generating interview questions and feedback\n",
        "* OpenAI Whisper ‚Äì to transcribe your audio answers\n",
        "* OpenAI TTS (Text-to-Speech) ‚Äì to hear the interview questions spoken aloud\n",
        "* Python ‚Äì for combining all components\n",
        "* Google Colab ‚Äì to run it all in your browser\n",
        "\n",
        "# üìã How to Use\n",
        "\n",
        "* üìÑ Paste your resume and the job description into the notebook.\n",
        "* üß† GPT-4 will generate customized interview questions.\n",
        "* üîä The AI will speak each question aloud using Text-to-Speech.\n",
        "* üéôÔ∏è You will record or upload your spoken answer.\n",
        "* üìù The assistant will transcribe your answer using Whisper.\n",
        "* üßæ GPT-4 will evaluate and respond with feedback or a follow-up question.\n",
        "* üîÅ Repeat as many times as you'd like.\n",
        "\n",
        "# ‚ö†Ô∏è Requirements\n",
        "\n",
        "* An OpenAI API Key with access to GPT-4, Whisper, and TTS\n",
        "* A microphone or audio file to upload\n",
        "* Basic understanding of English (intermediate+ recommended)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PlDwfyA7ltC"
      },
      "source": [
        "### 1. üß∞ Install and Import Required Libraries\n",
        "\n",
        "This cell installs and imports all the necessary Python and system libraries\n",
        "used throughout the notebook, including audio processing tools, display widgets,\n",
        "and the OpenAI API client.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wg8TsfwG7fa"
      },
      "outputs": [],
      "source": [
        "# üì¶ Install required libraries (only needs to be run once)\n",
        "!pip install --quiet pydub\n",
        "!apt-get install -y ffmpeg\n",
        "!pip install pymupdf ipywidgets\n",
        "\n",
        "# üì¶ Import required libraries\n",
        "import openai\n",
        "import os\n",
        "import IPython.display as ipd\n",
        "from google.colab import files\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "\n",
        "# üìö Additional utilities\n",
        "import tempfile\n",
        "import base64\n",
        "from IPython.display import HTML, Audio\n",
        "from getpass import getpass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hhE0ygGJTVX"
      },
      "source": [
        "### 2. üîê OpenAI API Key Setup\n",
        "\n",
        "In this step, you'll securely enter your OpenAI API key, which is required to access GPT-4, Whisper (speech-to-text), and TTS (text-to-speech).  \n",
        "The notebook will test your key by listing available models to confirm the connection is successful.\n",
        "\n",
        "üëâ You can get your API key from: [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHWK8pN_I82R"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from getpass import getpass\n",
        "\n",
        "# üîë Securely ask for the API key\n",
        "api_key = getpass(\"üîê Enter your OpenAI API Key: \")\n",
        "\n",
        "# üéØ Initialize the OpenAI client with your key\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# ‚úÖ Test the key by listing available models\n",
        "try:\n",
        "    models = client.models.list()\n",
        "    print(\"‚úÖ API key is valid. OpenAI models retrieved successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Failed to authenticate. Please check your API key.\")\n",
        "    print(\"Error:\", e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEczAHbR7_uW"
      },
      "source": [
        "### 3. üìÑ Resume Input Type Selection\n",
        "\n",
        "In this step, you'll choose how to provide your resume:\n",
        "- **Upload a PDF file** (recommended if your resume is already formatted)\n",
        "- **Paste the resume text manually** (if you prefer to copy and paste)\n",
        "\n",
        "After selecting an option, the notebook will display the appropriate input method in the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbo4g679LMqz"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "resume_input_type = widgets.RadioButtons(\n",
        "    options=[\"üìÑ Upload PDF file\", \"üìù Paste text manually\"],\n",
        "    value=\"üìÑ Upload PDF file\",\n",
        "    description='Resume:',\n",
        ")\n",
        "\n",
        "display(resume_input_type)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X23mTFsb8Krp"
      },
      "source": [
        "### 4. üì• Upload Resume File or Paste Text\n",
        "\n",
        "This step loads your resume based on the selected method:\n",
        "- If you chose **PDF upload**, the file will be uploaded and the text automatically extracted.\n",
        "- If you chose **manual input**, a text box will appear for you to paste your resume content.\n",
        "\n",
        "The extracted or pasted text will be stored in a variable for use in the interview simulation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr8qibAMPeF0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "if resume_input_type.value == \"üìù Paste text manually\":\n",
        "    resume_textarea = widgets.Textarea(\n",
        "        placeholder='Paste your resume here...',\n",
        "        description='Text:',\n",
        "        layout=widgets.Layout(width='100%', height='200px')\n",
        "    )\n",
        "    display(resume_textarea)\n",
        "    resume_text = resume_textarea\n",
        "else:\n",
        "    print(\"üìÑ Please upload your resume PDF file below:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Read and extract text from the uploaded PDF\n",
        "    for fname in uploaded:\n",
        "        doc = fitz.open(fname)\n",
        "        resume_text = \"\"\n",
        "        for page in doc:\n",
        "            resume_text += page.get_text()\n",
        "    print(\"‚úÖ Resume successfully loaded from PDF.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IoAlwT98dk_"
      },
      "source": [
        "### 5. üíº Paste and Confirm the Job Description\n",
        "\n",
        "In this step, paste the **job description** you want to simulate the interview for.\n",
        "\n",
        "After pasting, click the **‚úÖ Confirm Job Description** button to store the content.  \n",
        "This description will be used along with your resume to generate customized interview questions based on the target role.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqokkWDBQROa"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Create TextArea for job description\n",
        "job_textarea = widgets.Textarea(\n",
        "    placeholder='Paste the job description here...',\n",
        "    description='Job:',\n",
        "    layout=widgets.Layout(width='100%', height='200px')\n",
        ")\n",
        "\n",
        "# Create Confirm button\n",
        "confirm_button = widgets.Button(\n",
        "    description='‚úÖ Confirm Job Description',\n",
        "    button_style='success',\n",
        "    tooltip='Click to store the job description',\n",
        ")\n",
        "\n",
        "# Output area to show result\n",
        "output = widgets.Output()\n",
        "\n",
        "# Handle button click\n",
        "def on_confirm_clicked(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        global job_description  # To make it accessible outside this cell\n",
        "        job_description = job_textarea.value\n",
        "        print(\"‚úÖ Job description stored successfully!\")\n",
        "\n",
        "# Attach handler\n",
        "confirm_button.on_click(on_confirm_clicked)\n",
        "\n",
        "# Display everything\n",
        "display(job_textarea, confirm_button, output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SUPUpz78oBs"
      },
      "source": [
        "### 6. üß† Generate the First Interview Question with GPT-4\n",
        "\n",
        "This step uses GPT-4 to generate the **first interview question**, tailored to your resume and the job description.\n",
        "\n",
        "The assistant takes the role of a professional interviewer and focuses on assessing your fit for the role.  \n",
        "Only one question will be asked at this stage ‚Äî follow-up questions will come later based on your answers.\n",
        "\n",
        "The question will be stored and used to start the ongoing interview session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54UpVKjrQbV-"
      },
      "outputs": [],
      "source": [
        "# üß† Prompt to GPT-4 to act as an interviewer\n",
        "prompt = f\"\"\"\n",
        "You are a professional English-speaking job interviewer conducting a realistic job interview.\n",
        "\n",
        "Your goals are:\n",
        "- Evaluate the candidate‚Äôs professional background and technical fit for the role.\n",
        "- Assess soft skills, behavior, and cultural fit.\n",
        "- Ask essential classic interview questions during the conversation, including:\n",
        "  - Tell me about yourself.\n",
        "  - What do you know about our company?\n",
        "  - Why do you want this job?\n",
        "  - Why should we hire you?\n",
        "  - What are your strengths and weaknesses?\n",
        "  - What makes a great [insert job title]?\n",
        "  - What would excellent performance look like?\n",
        "\n",
        "Important rules:\n",
        "- Ask only one question at a time.\n",
        "- Do not answer the questions yourself.\n",
        "- Be professional but natural.\n",
        "- Alternate between technical/job-fit questions and general/classic questions.\n",
        "\n",
        "Resume:\n",
        "{resume_text}\n",
        "\n",
        "Job Description:\n",
        "{job_description}\n",
        "\n",
        "Please ask the first interview question now.\n",
        "\"\"\"\n",
        "\n",
        "# Create message history for ongoing conversation\n",
        "chat_history = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a professional English-speaking job interviewer.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "# üîÅ Get the first interview question\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=chat_history,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# üó£Ô∏è Store and display the first question\n",
        "first_question = response.choices[0].message.content\n",
        "chat_history.append({\"role\": \"assistant\", \"content\": first_question})\n",
        "print(\"üó®Ô∏è Interviewer:\", first_question)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCnz1CfhSKtl"
      },
      "source": [
        "### 7. üîä Convert Interview Question to Audio (Text-to-Speech)\n",
        "\n",
        "This step uses **OpenAI‚Äôs Text-to-Speech (TTS)** engine to generate **natural-sounding audio** from the first interview question.\n",
        "\n",
        "You‚Äôll hear the question spoken aloud, simulating a real interview scenario.  \n",
        "The voice model used here is `\"alloy\"`, but you can experiment with other options like: `echo`, `fable`, `onyx`, `nova`, or `shimmer`.\n",
        "\n",
        "üëâ Make sure your speakers or headphones are on!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiNnXp-k86s9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_C4-GC-SSeB"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from IPython.display import Audio\n",
        "\n",
        "# üó£Ô∏è Generate audio from the first interview question using OpenAI TTS\n",
        "speech_response = client.audio.speech.create(\n",
        "    model=\"tts-1\",\n",
        "    voice=\"alloy\",  # Other options: 'echo', 'fable', 'onyx', 'nova', 'shimmer'\n",
        "    input=first_question\n",
        ")\n",
        "\n",
        "# üíæ Save audio to a file\n",
        "audio_path = \"first_question.mp3\"\n",
        "with open(audio_path, \"wb\") as f:\n",
        "    f.write(speech_response.content)\n",
        "\n",
        "# ‚ñ∂Ô∏è Play the audio in notebook\n",
        "display(Audio(audio_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBoeG7Wn9GF4"
      },
      "source": [
        "### 8. üéôÔ∏è Record Your Answer, Continue the Interview, and Export Transcript\n",
        "\n",
        "This cell launches the **interactive microphone interface** with three buttons:\n",
        "\n",
        "- ‚ñ∂Ô∏è **Start Recording** ‚Äì begins recording your spoken response  \n",
        "- ‚èπÔ∏è **Stop Recording** ‚Äì stops and saves the audio  \n",
        "- üõë **End Interview** ‚Äì ends the session and downloads the full transcript\n",
        "\n",
        "After each recording:\n",
        "1. Your voice is transcribed into text using **Whisper**\n",
        "2. GPT-4 reads your answer and generates a **follow-up question**\n",
        "3. The question is spoken aloud using **Text-to-Speech**\n",
        "4. The conversation is stored in `chat_history` for context\n",
        "\n",
        "At the end, a complete Markdown transcript is generated and offered as a **downloadable link**, so you can review or share your simulated interview.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBNRej74TQHf"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Javascript, display, Audio, FileLink\n",
        "from google.colab import output\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Updated JS with blinking and state handling\n",
        "RECORD_JS = \"\"\"\n",
        "const startBtn = document.createElement(\"button\");\n",
        "const stopBtn = document.createElement(\"button\");\n",
        "const endBtn = document.createElement(\"button\");\n",
        "\n",
        "startBtn.textContent = \"‚ñ∂Ô∏è Start Recording\";\n",
        "stopBtn.textContent = \"‚èπÔ∏è Stop Recording\";\n",
        "endBtn.textContent = \"üõë End Interview\";\n",
        "\n",
        "startBtn.style = stopBtn.style = endBtn.style = \"margin: 10px; padding: 10px; font-size: 16px;\";\n",
        "startBtn.style.animation = \"\";\n",
        "\n",
        "document.body.appendChild(startBtn);\n",
        "document.body.appendChild(stopBtn);\n",
        "document.body.appendChild(endBtn);\n",
        "\n",
        "const sleep = time => new Promise(resolve => setTimeout(resolve, time));\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader();\n",
        "  reader.onloadend = () => resolve(reader.result);\n",
        "  reader.readAsDataURL(blob);\n",
        "});\n",
        "\n",
        "let stream;\n",
        "let recorder;\n",
        "let chunks = [];\n",
        "\n",
        "startBtn.onclick = async () => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  chunks = [];\n",
        "  recorder.ondataavailable = e => chunks.push(e.data);\n",
        "  recorder.start();\n",
        "\n",
        "  startBtn.textContent = \"üî¥ Recording...\";\n",
        "  startBtn.style.animation = \"blinker 1s linear infinite\";\n",
        "  startBtn.style.color = \"red\";\n",
        "};\n",
        "\n",
        "stopBtn.onclick = async () => {\n",
        "  recorder.stop();\n",
        "  await new Promise(resolve => recorder.onstop = resolve);\n",
        "  stream.getTracks().forEach(track => track.stop());\n",
        "\n",
        "  startBtn.textContent = \"‚ñ∂Ô∏è Start Recording\";\n",
        "  startBtn.style.animation = \"\";\n",
        "  startBtn.style.color = \"\";\n",
        "\n",
        "  let blob = new Blob(chunks);\n",
        "  let base64 = await b2text(blob);\n",
        "  google.colab.kernel.invokeFunction(\"notebook.audio_result\", [base64], {});\n",
        "};\n",
        "\n",
        "endBtn.onclick = () => {\n",
        "  google.colab.kernel.invokeFunction(\"notebook.end_interview\", [], {});\n",
        "};\n",
        "\n",
        "// CSS blinking animation\n",
        "const style = document.createElement(\"style\");\n",
        "style.textContent = `\n",
        "@keyframes blinker {\n",
        "  50% { opacity: 0.2; }\n",
        "}`;\n",
        "document.head.appendChild(style);\n",
        "\"\"\"\n",
        "\n",
        "# Callback: Process audio and continue the chat\n",
        "def handle_audio(base64_audio):\n",
        "    audio_data = base64.b64decode(base64_audio.split(',')[1])\n",
        "    audio_path = \"user_response.wav\"\n",
        "    with open(audio_path, \"wb\") as f:\n",
        "        f.write(audio_data)\n",
        "    print(\"‚úÖ Audio recorded. Transcribing...\")\n",
        "\n",
        "    transcription = client.audio.transcriptions.create(\n",
        "        model=\"whisper-1\",\n",
        "        file=open(audio_path, \"rb\"),\n",
        "        response_format=\"text\"\n",
        "    )\n",
        "    user_answer = transcription.strip()\n",
        "    print(\"üìù You said:\", user_answer)\n",
        "\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_answer})\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=chat_history,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    next_question = response.choices[0].message.content\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": next_question})\n",
        "    print(\"\\nüó®Ô∏è Interviewer:\", next_question)\n",
        "\n",
        "    speech = client.audio.speech.create(\n",
        "        model=\"tts-1\",\n",
        "        voice=\"alloy\",\n",
        "        input=next_question\n",
        "    )\n",
        "    with open(\"next_question.mp3\", \"wb\") as f:\n",
        "        f.write(speech.content)\n",
        "\n",
        "    display(Audio(\"next_question.mp3\"))\n",
        "\n",
        "# Callback: Export and offer full transcript\n",
        "def end_interview():\n",
        "    print(\"üìÑ Generating transcript...\")\n",
        "\n",
        "    md_lines = [\"# üß† Interview Transcript\\n\"]\n",
        "    for entry in chat_history:\n",
        "        role = \"üë§ You\" if entry[\"role\"] == \"user\" else \"üó®Ô∏è Interviewer\"\n",
        "        md_lines.append(f\"**{role}:** {entry['content']}\\n\")\n",
        "\n",
        "    md_text = \"\\n\".join(md_lines)\n",
        "\n",
        "    # Save the file locally\n",
        "    with open(\"interview_transcript.md\", \"w\") as f:\n",
        "        f.write(md_text)\n",
        "\n",
        "    # Encode for browser-safe download link\n",
        "    b64 = base64.b64encode(md_text.encode()).decode()\n",
        "    download_link = f'<a download=\"interview_transcript.md\" href=\"data:text/markdown;base64,{b64}\" target=\"_blank\">üì• Click here to download your interview transcript</a>'\n",
        "\n",
        "    display(HTML(download_link))\n",
        "\n",
        "# Register both callbacks\n",
        "output.register_callback(\"notebook.audio_result\", handle_audio)\n",
        "output.register_callback(\"notebook.end_interview\", end_interview)\n",
        "\n",
        "# Launch interface\n",
        "display(Javascript(RECORD_JS))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME5F7aLZF5zk0TXYrUNi31",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}